{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import random\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "tqdm.pandas(desc='progress-bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english') + list(string.punctuation))\n",
    "stemmer = PorterStemmer()\n",
    "re_punct = re.compile('[' + ''.join(string.punctuation) + ']')\n",
    "\n",
    "def preprocess(text):\n",
    "    try:\n",
    "        text = text.lower()\n",
    "        tokens = word_tokenize(text)\n",
    "#         tokens = [t for t in tokens if not t in stop]\n",
    "        tokens = [re.sub(re_punct, ' ', t) for t in tokens]\n",
    "        tokens = [t for t in tokens if len(t) > 2]\n",
    "#         tokens = [stemmer.stem(t) for t in tokens]\n",
    "        if len(tokens) == 0:\n",
    "            return ''\n",
    "        else:\n",
    "            return ' '.join(tokens)\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "def tokenize_count(row):\n",
    "    return len(row.split(' '))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_splits(X, y, max_words = 20000, MAX_SEQUENCE_LENGTH = 100, EMBEDDING_DIM = 300):\n",
    "    tokenizer = Tokenizer(num_words=max_words,lower=True, split=' ', \n",
    "                          filters='\"#%&()*+-/<=>@[\\\\]^_`{|}~\\t\\n',\n",
    "                          char_level=False, oov_token=u'<UNK>')\n",
    "\n",
    "    tokenizer.fit_on_texts(X)\n",
    "\n",
    "    X = tokenizer.texts_to_sequences(X)\n",
    "    Y = pd.get_dummies(y).values\n",
    "    # print(X[0])\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH,  padding=\"post\", truncating=\"post\")\n",
    "    # print(X[10])\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, \n",
    "                                                        random_state = 42)\n",
    "\n",
    "    # print(y_train[100])\n",
    "\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print('x_test shape:', x_test.shape)\n",
    "    print('y_train shape:', y_train.shape)\n",
    "    print('y_test shape:', y_test.shape)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|██████████| 422/422 [00:01<00:00, 225.43it/s]\n",
      "progress-bar: 100%|██████████| 422/422 [00:00<00:00, 6616.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>title</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>label</th>\n",
       "      <th>collected_by</th>\n",
       "      <th>token_count</th>\n",
       "      <th>title_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>Bobby Knight: Nobody can 'correct mistakes' li...</td>\n",
       "      <td>bobby knight nobody can  correct mistakes like...</td>\n",
       "      <td>Bobby Knight: Nobody can 'correct mistakes' li...</td>\n",
       "      <td>bobby knight nobody can  correct mistakes like...</td>\n",
       "      <td>RealNewsContent</td>\n",
       "      <td>BuzzFeed</td>\n",
       "      <td>352</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>Bono: ‘Trump is potentially the worst idea tha...</td>\n",
       "      <td>bono ‘trump potentially the worst idea that ev...</td>\n",
       "      <td>Bono: ‘Trump is potentially the worst idea tha...</td>\n",
       "      <td>bono ‘trump potentially the worst idea that ev...</td>\n",
       "      <td>RealNewsContent</td>\n",
       "      <td>BuzzFeed</td>\n",
       "      <td>142</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>Story highlights \"This, though, is certain: to...</td>\n",
       "      <td>story highlights this though certain too many ...</td>\n",
       "      <td>Hillary Clinton on police shootings: 'too many...</td>\n",
       "      <td>hillary clinton police shootings  too many peo...</td>\n",
       "      <td>RealNewsContent</td>\n",
       "      <td>BuzzFeed</td>\n",
       "      <td>220</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>Hillary Clinton and Donald Trump ushered the 2...</td>\n",
       "      <td>hillary clinton and donald trump ushered the 2...</td>\n",
       "      <td>10 Moments That Mattered From Hillary Clinton ...</td>\n",
       "      <td>moments that mattered from hillary clinton and...</td>\n",
       "      <td>RealNewsContent</td>\n",
       "      <td>BuzzFeed</td>\n",
       "      <td>1440</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>Story highlights Ryan called for calm in Charl...</td>\n",
       "      <td>story highlights ryan called for calm charlott...</td>\n",
       "      <td>Paul Ryan ducks on stop-and-frisk</td>\n",
       "      <td>paul ryan ducks stop and frisk</td>\n",
       "      <td>RealNewsContent</td>\n",
       "      <td>BuzzFeed</td>\n",
       "      <td>157</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "417  Bobby Knight: Nobody can 'correct mistakes' li...   \n",
       "418  Bono: ‘Trump is potentially the worst idea tha...   \n",
       "419  Story highlights \"This, though, is certain: to...   \n",
       "420  Hillary Clinton and Donald Trump ushered the 2...   \n",
       "421  Story highlights Ryan called for calm in Charl...   \n",
       "\n",
       "                                           text_tokens  \\\n",
       "417  bobby knight nobody can  correct mistakes like...   \n",
       "418  bono ‘trump potentially the worst idea that ev...   \n",
       "419  story highlights this though certain too many ...   \n",
       "420  hillary clinton and donald trump ushered the 2...   \n",
       "421  story highlights ryan called for calm charlott...   \n",
       "\n",
       "                                                 title  \\\n",
       "417  Bobby Knight: Nobody can 'correct mistakes' li...   \n",
       "418  Bono: ‘Trump is potentially the worst idea tha...   \n",
       "419  Hillary Clinton on police shootings: 'too many...   \n",
       "420  10 Moments That Mattered From Hillary Clinton ...   \n",
       "421                  Paul Ryan ducks on stop-and-frisk   \n",
       "\n",
       "                                          title_tokens            label  \\\n",
       "417  bobby knight nobody can  correct mistakes like...  RealNewsContent   \n",
       "418  bono ‘trump potentially the worst idea that ev...  RealNewsContent   \n",
       "419  hillary clinton police shootings  too many peo...  RealNewsContent   \n",
       "420  moments that mattered from hillary clinton and...  RealNewsContent   \n",
       "421                     paul ryan ducks stop and frisk  RealNewsContent   \n",
       "\n",
       "    collected_by  token_count  title_count  \n",
       "417     BuzzFeed          352            9  \n",
       "418     BuzzFeed          142           10  \n",
       "419     BuzzFeed          220           17  \n",
       "420     BuzzFeed         1440           11  \n",
       "421     BuzzFeed          157            6  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def prep_fakeNewsNet():\n",
    "    dfs = []\n",
    "    for root, dirs, files in os.walk('data/FakeNewsNet/Data/'): \n",
    "        for filename in files:\n",
    "            if filename.endswith('json'):\n",
    "    #             print('%s/%s' % (root, filename))\n",
    "                with open('%s/%s' % (root, filename)) as json_data:\n",
    "                    data = json.load(json_data)\n",
    "                    df = pd.DataFrame.from_dict(data, orient='index').T\n",
    "                    df['label'] = root.split('/')[4]\n",
    "                    df['collected_by'] = root.split('/')[3]\n",
    "                    dfs.append(df)\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)#.set_index('number')   \n",
    "    df['text_tokens'] = df['text'].progress_map(preprocess)\n",
    "    df['title_tokens'] = df['title'].progress_map(preprocess)\n",
    "    df = df[['text', 'text_tokens', 'title', 'title_tokens', 'label', 'collected_by']]\n",
    "    df['token_count'] = df['text_tokens'].apply(tokenize_count)\n",
    "    df['title_count'] = df['title_tokens'].apply(tokenize_count)\n",
    "    return df\n",
    "\n",
    "def get_fakeNewsNet():\n",
    "    data_train = pd.read_csv('data/fakeNewsNet.csv', sep='\\t')\n",
    "    data_train['label'].value_counts().plot(kind='bar', alpha=.5)\n",
    "\n",
    "    print('Loading data...')\n",
    "\n",
    "    X = data_train['tokens'].values\n",
    "    y = data_train['label']\n",
    " \n",
    "    labels = data_train['label'].unique()\n",
    "    print(labels)\n",
    "    \n",
    "    num_classes = len(labels)\n",
    "    print(num_classes, 'classes')\n",
    "    \n",
    "    x_train, x_test, y_train, y_test, word_index = get_splits(X, y)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, word_index, labels, num_classes, X, y\n",
    "\n",
    "\n",
    "# df = prep_fakeNewsNet()\n",
    "# df.to_csv('data/fakeNewsNet.csv', sep='\\t')\n",
    "# df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_fake_or_real_news():\n",
    "    data_train = pd.read_csv('data/fake_or_real_news.csv')\n",
    "    data_train['tokens'] = data_train['text'].progress_map(preprocess)\n",
    "    \n",
    "    data_train = data_train[data_train['tokens'].notnull()]\n",
    "    data_train.reset_index(inplace=True)\n",
    "    data_train.drop('index', inplace=True, axis=1)\n",
    "    data_train.to_csv('data/fake_or_real_news.csv')\n",
    "\n",
    "# prep_fake_or_real_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fake_or_real_news():\n",
    "    data_train = pd.read_csv('data/fake_or_real_news.csv')\n",
    "    data_train['label'].value_counts().plot(kind='bar', alpha=.5)\n",
    "\n",
    "    print('Loading data...')\n",
    "\n",
    "    X = data_train['tokens'].values\n",
    "    y = data_train['label']\n",
    "\n",
    "    classes = data_train['label'].unique()\n",
    "    \n",
    "    labels = data_train['label'].unique()\n",
    "    print(labels)\n",
    "    \n",
    "    num_classes = len(labels)\n",
    "    print(num_classes, 'classes')\n",
    "    \n",
    "    x_train, x_test, y_train, y_test, word_index = get_splits(X, y)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, word_index, labels, num_classes, X, y\n",
    "\n",
    "\n",
    "# x_train, x_test, y_train, y_test, word_index, labels, num_classes = get_fake_or_real_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_politifact():\n",
    "    data_train = pd.read_csv('data/politifact.tsv', sep='\\t')\n",
    "\n",
    "    data_train['ruling'].value_counts().plot(kind='bar', alpha=.5)\n",
    "\n",
    "    data_train['label'] = '' \n",
    "    def label(row):\n",
    "    #     print(row)\n",
    "        if row in ['true', 'half-true', 'mostly-true']:\n",
    "            return 'true'\n",
    "        else:\n",
    "            return 'false'\n",
    "\n",
    "    data_train['label'] = data_train['ruling'].apply(label)\n",
    "\n",
    "    print('Loading data...')\n",
    "    \n",
    "        ###############\n",
    "    X  = data_train['statement__text'].values\n",
    "    y = data_train['label']\n",
    "\n",
    "    classes = data_train['label'].unique()\n",
    "    \n",
    "    labels = data_train['label'].unique()\n",
    "    print(labels)\n",
    "    \n",
    "    num_classes = len(labels)\n",
    "    print(num_classes, 'classes')\n",
    "    \n",
    "    x_train, x_test, y_train, y_test, word_index = get_splits(X, y)\n",
    "\n",
    "    data_train['label'].value_counts().plot(kind='bar', alpha=.5)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, word_index, labels, num_classes, X, y\n",
    " \n",
    "# x_train, x_test, y_train, y_test, word_index, labels, num_classes = get_politifact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prep_UCI():\n",
    "    header = ['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME','TIMESTAMP']\n",
    "    # b = business, t = science and technology, e = entertainment, m = health\n",
    "    data_train = pd.read_csv('data/newsCorpora.csv', names=header,  sep='\\t')\n",
    "    \n",
    "    data_train['TITLE'] = data_train['TITLE'].progress_map(preprocess)\n",
    "\n",
    "    data_train = data_train[data_train['TITLE'].notnull()]\n",
    "    data_train.drop('ID', inplace=True, axis=1)\n",
    "    data_train.reset_index(inplace=True)\n",
    "    data_train['token_count'] = data_train['TITLE'].apply(tokenize_count)\n",
    "    data_train = data_train[data_train['token_count'] < 50]\n",
    "    \n",
    "    return data_train\n",
    "    \n",
    "# data_train = prep_UCI()\n",
    "# data_train['CATEGORY'].value_counts().plot(kind='bar', alpha=.5)\n",
    "# data_train['token_count'].hist(bins=10, alpha=.5)\n",
    "# # data_train.to_csv('data/newsCorpora.csv', sep='\\t')\n",
    "# data_train.tail()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data_train['token_count'].hist(bins=10, alpha=.5)\n",
    "# data_train['token_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prep_fake():\n",
    "    data_train = pd.read_csv('data/fake.csv')\n",
    "    data_train['text'] = data_train['text'].progress_map(preprocess)\n",
    "    data_train['thread_title'] = data_train['thread_title'].progress_map(preprocess)\n",
    "\n",
    "    data_train = data_train[data_train['thread_title'].notnull()]\n",
    "    data_train.reset_index(inplace=True)\n",
    "    data_train.drop('index', inplace=True, axis=1)\n",
    "    data_train['token_count'] = data_train['thread_title'].apply(tokenize_count)\n",
    "    return data_train\n",
    "\n",
    "# fake_df = prep_fake()\n",
    "# fake_df.to_csv('data/fake.csv')\n",
    "# fake_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake_df['token_count'].hist(bins=10, alpha=.5)\n",
    "# fake_df['token_count'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fact_fake():\n",
    "    # print('Loading data...')\n",
    "\n",
    "    fake_df = pd.read_csv('data/fake.csv')\n",
    "    real_df = pd.read_csv('data/newsCorpora.csv', sep='\\t')\n",
    "    text = np.append(fake_df['thread_title'].values,  real_df['TITLE'].values)\n",
    "    label = np.append( ['FALSE'] * len(fake_df), ['TRUE'] * len(real_df))\n",
    "    data = {'text' : text, 'label': label}\n",
    "\n",
    "    data_train = pd.DataFrame(data)\n",
    "    data_train['label'].value_counts().plot(kind='bar', alpha=.5)\n",
    "    print(data_train['label'].value_counts())\n",
    "    data_train.tail()\n",
    "\n",
    "    X  = data_train['text'].values\n",
    "    y = data_train['label']\n",
    "\n",
    "    labels = data_train['label'].unique()\n",
    "    print(labels)\n",
    "\n",
    "    num_classes = len(labels)\n",
    "    print(num_classes, 'classes')\n",
    "\n",
    "    x_train, x_test, y_train, y_test, word_index = get_splits(X, y)\n",
    "\n",
    "#     data_train['label'].value_counts().plot(kind='bar', alpha=.5)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test, word_index, labels, num_classes, X, y\n",
    "\n",
    "# x_train, x_test, y_train, y_test, word_index, labels, num_classes, _ = get_fact_fake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
