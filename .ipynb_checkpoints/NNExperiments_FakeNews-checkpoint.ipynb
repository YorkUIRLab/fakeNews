{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import Bidirectional, TimeDistributed, GRU\n",
    "from keras.layers import LSTM, Input, Reshape, Concatenate, Flatten,Convolution1D\n",
    "from keras.layers import Conv1D, Conv2D, GlobalMaxPooling1D, MaxPooling1D, MaxPool2D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from os.path import expanduser, exists\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "# pd.set_option('display.max_colwidth', 300) #widen pandas rows display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('data/hansard_processed_all.csv')\n",
    "from sklearn.utils import shuffle\n",
    "data_train = shuffle(data_train).reset_index(drop=True)\n",
    "\n",
    "# filter for experiment\n",
    "# data_train = data_train[:50000]\n",
    "\n",
    "labels = data_train['label'].unique()\n",
    "print(labels)\n",
    "data_train['label'].value_counts().plot(kind='bar', alpha=.5)\n",
    "data_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_confusion_matrix(model, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    "    y_pred = model.predict(x_test)\n",
    "    # print(y_pred[:10])\n",
    "    # print(y_test[:10])\n",
    "    class_labels = np.argmax(y_test, axis=1) \n",
    "    # print(class_labels[:10])\n",
    "    # print(y_pred.argmax(axis=1))\n",
    "    print(metrics.classification_report(class_labels, y_pred.argmax(axis=1), \n",
    "                                        target_names=data_train['label'].unique(), digits=3))\n",
    "    \n",
    "    cm = confusion_matrix(class_labels, \n",
    "                          y_pred.argmax(axis=1))\n",
    "    \n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "#     plt.figure(figsize=(12,8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "EMBEDDING_DIM = 300 \n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words,lower=True, split=' ', \n",
    "                      filters='\"#%&()*+-/<=>@[\\\\]^_`{|}~\\t\\n',\n",
    "                      char_level=False, oov_token=u'<UNK>')\n",
    "\n",
    "tokenizer.fit_on_texts(data_train['TokenizedContent'].values)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(data_train['TokenizedContent'].values)\n",
    "Y = pd.get_dummies(data_train['label']).values\n",
    "# print(X[0])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH,  padding=\"post\", truncating=\"post\")\n",
    "# print(X[10])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "# print(y_train[100])\n",
    "\n",
    "num_classes = y_train.shape[1]\n",
    "print(num_classes, 'classes')\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "KERAS_DATASETS_DIR = expanduser('~/.keras/datasets/')\n",
    "GLOVE_FILE = 'glove.840B.300d.txt'\n",
    "\n",
    "print(\"Processing\", GLOVE_FILE)\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(KERAS_DATASETS_DIR + GLOVE_FILE, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings: %d' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_layer():\n",
    "    count = 0\n",
    "    embedding_matrix = np.random.uniform(-1,0,(len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            count += 1\n",
    "            \n",
    "    print('Word embeddings: %d' % len(embeddings_index))\n",
    "    print('found number of tokens in embedding space: ', count)\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "    return embedding_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "\n",
    "epochs=50\n",
    "batch_size=128\n",
    "# configure problem\n",
    "n_features = 50\n",
    "n_timesteps_in = 5\n",
    "n_timesteps_out = 2\n",
    "\n",
    "def encoder_decoder_baseline():\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(get_embedding_layer())\n",
    "    model.add(LSTM(150, dropout=0.2, recurrent_dropout=0.2))\n",
    "#     model.add(LSTM(150, input_shape=(n_timesteps_in, n_features)))\n",
    "    model.add(RepeatVector(n_timesteps_in))\n",
    "    model.add(LSTM(150, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(MAX_SEQUENCE_LENGTH, activation='softmax')))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "model = encoder_decoder_baseline()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "network_hist = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(score)\n",
    "\n",
    "plot_history(network_hist)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Very hopefull... work on this\n",
    "# https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
    "epochs=50\n",
    "batch_size=128\n",
    "\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "def word_embedding_model():\n",
    "    print('building word embedding model')\n",
    "    embedding_layer = get_embedding_layer()\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Conv1D(64, 5, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(64, 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(64, 5, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    preds = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = word_embedding_model()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "network_hist = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(score)\n",
    "print(model.metrics_names)\n",
    "\n",
    "plot_history(network_hist)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "# 84% 85%\n",
    "# epochs = 20\n",
    "# batch_size = 64\n",
    "\n",
    "def imbd_lstm():\n",
    "    print('Build LSTM model...')\n",
    "    model = Sequential()\n",
    "    model.add(get_embedding_layer())\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='RMSprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = imbd_lstm()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "print('Train...')\n",
    "network_hist = model.fit(x_train, y_train, \n",
    "                         batch_size=batch_size, epochs=epochs,\n",
    "                         verbose=1, validation_data=(x_test, y_test),\n",
    "                         validation_split=0.2)\n",
    "\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "plot_history(network_hist)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# https://richliao.github.io/supervised/classification/2016/12/26/textclassifier-RNN/\n",
    "# 81%\n",
    "# epochs=10\n",
    "batch_size=128\n",
    "\n",
    "def getRNN_classifier():\n",
    "    embedding_layer = get_embedding_layer()\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    l_lstm = Bidirectional(LSTM(128))(embedded_sequences)\n",
    "    preds = Dense(num_classes, activation='softmax')(l_lstm)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = getRNN_classifier()\n",
    "\n",
    "    \n",
    "print(\"model fitting - Bidirectional LSTM\")\n",
    "model.summary()\n",
    "\n",
    "network_hist = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "plot_history(network_hist)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# https://github.com/Theo-/sentiment-analysis-keras-conv/blob/master/train_keras.py\n",
    "\n",
    "def sentiment_keras():\n",
    "    model = Sequential()\n",
    "\n",
    "    embedding_layer = get_embedding_layer()\n",
    "    model.add(embedding_layer)\n",
    "\n",
    "    # Convolutional model (3x conv, flatten, 2x dense)\n",
    "    model.add(Convolution1D(64, 3, padding='same'))\n",
    "    model.add(Convolution1D(32, 3, padding='same'))\n",
    "    model.add(Convolution1D(16, 3, padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(180, activation='sigmoid'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = sentiment_keras()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "network_hist = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    epochs=epochs, verbose=1,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "plot_history(network_hist)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# https://github.com/keras-team/keras/blob/master/examples/imdb_cnn.py\n",
    "# 81% 82%\n",
    "# set parameters:\n",
    "batch_size = 256\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "# epochs = 10\n",
    "\n",
    "def imbd_cnn():\n",
    "\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "#     model.add(get_embedding_layer())\n",
    "    model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)TimeDistributed)\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # we add a Convolution1D, which will learn filters\n",
    "    # word group filters of size filter_length:\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    # we use max pooling:\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(hidden_dims))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='RMSprop',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "model = imbd_cnn()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "print('Training...')\n",
    "network_hist = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs, validation_split=0.2,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "plot_history(network_hist)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# https://github.com/keras-team/keras/blob/master/examples/imdb_cnn_lstm.py\n",
    "# 85% - 82% -85% 86%\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 128\n",
    "# epochs = 20\n",
    "\n",
    "def LSTMModel():\n",
    "    print('Build model...')\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(get_embedding_layer())\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(LSTM(lstm_output_size))\n",
    "\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='RMSprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = LSTMModel()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "print('Train...')\n",
    "network_hist = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs, validation_split=0.2,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "plot_history(network_hist)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# https://github.com/bhaveshoswal/CNN-text-classification-keras/blob/master/model.py\n",
    "# 84% - %83 - 83%\n",
    "vocabulary_size = len(word_index) \n",
    "# embedding_dim = 100\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "drop = 0.5\n",
    "\n",
    "# epochs = 20\n",
    "# batch_size = 10\n",
    "\n",
    "def CNNTextClassification():\n",
    "    # this returns a tensor\n",
    "    print(\"Creating Model...\")\n",
    "    inputs = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedding_layer = get_embedding_layer()\n",
    "    embedding = embedding_layer(inputs)\n",
    "    # Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)\n",
    "\n",
    "    reshape = Reshape((MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, 1))(embedding)\n",
    "\n",
    "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], EMBEDDING_DIM), \n",
    "                    padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], EMBEDDING_DIM), \n",
    "                    padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "    conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], EMBEDDING_DIM), \n",
    "                    padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "    maxpool_0 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[0] + 1, 1), \n",
    "                          strides=(1,1), padding='valid')(conv_0)\n",
    "    maxpool_1 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[1] + 1, 1), \n",
    "                          strides=(1,1), padding='valid')(conv_1)\n",
    "    maxpool_2 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[2] + 1, 1), \n",
    "                          strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "    concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "    flatten = Flatten()(concatenated_tensor)\n",
    "    dropout = Dropout(drop)(flatten)\n",
    "    output = Dense(units=num_classes, activation='softmax')(dropout)\n",
    "\n",
    "    # this creates a model that includes\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    checkpoint = ModelCheckpoint('weights.{epoch:03d}-{val_acc:.4f}.hdf5', \n",
    "                                 monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(optimizer=adam, \n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = CNNTextClassification()\n",
    "print(model.summary())\n",
    "\n",
    "print(\"Traning Model...\")\n",
    "network_hist = model.fit(x_train, y_train, batch_size=batch_size, \n",
    "                         epochs=epochs, verbose=1, \n",
    "                         validation_data=(x_test, y_test))  # starts training\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "plot_history(network_hist)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras convolutional model\n",
    "# 80% 85% 85%\n",
    "\n",
    "# batch_size = 10\n",
    "epochs = 30\n",
    "\n",
    "def keras_conv():\n",
    "    model = Sequential()\n",
    "    model.add(get_embedding_layer())\n",
    "    model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "    model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "    model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "    model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "    model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "    model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "    model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # model.add(Dense(256, activation='tanh'))\n",
    "    model.add(Dense(256, activation='sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "    model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "    adam = Adam(lr=0.0001, decay=1e-6)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "mdoel = keras_conv()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "print('Training...')\n",
    "# Fit the model\n",
    "network_hist = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          shuffle=True,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[EarlyStopping(min_delta=0.00025, patience=2)])\n",
    "\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "plot_history(network_hist)\n",
    "\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# simple \n",
    "# epochs = 20\n",
    "\n",
    "# 84%, 82%, 83%\n",
    "\n",
    "def simple_model():\n",
    "    # define model\n",
    "    print('simpel model')\n",
    "    model = Sequential()\n",
    "    model.add(get_embedding_layer())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer=adam, \n",
    "                  loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "model = simple_model()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "print('Training...')\n",
    "# Fit the model\n",
    "network_hist = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          shuffle=True,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "plot_history(network_hist)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://richliao.github.io/supervised/classification/2016/12/26/textclassifier-RNN/\n",
    "# 83% 83% 86%\n",
    "epochs = 20\n",
    "\n",
    "def RNN_model():\n",
    "    embedding_layer = get_embedding_layer()\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    l_lstm = Bidirectional(LSTM(100))(embedded_sequences)\n",
    "\n",
    "    preds = Dense(num_classes, activation='softmax')(l_lstm)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = RNN_model()\n",
    "print(\"model fitting - Bidirectional LSTM\")\n",
    "print(model.summary())\n",
    "\n",
    "network_hist = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          shuffle=True,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "plot_history(network_hist)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    embedding_layer = get_embedding_layer()\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    \n",
    "    model.add(Dense(128, input_dim=MAX_SEQUENCE_LENGTH, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "#     print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "def GRUModel():\n",
    "    print('Build model...')\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(get_embedding_layer())\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(GRU(lstm_output_size))\n",
    "\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='RMSprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = GRUModel()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "def test(model, data):\n",
    "    x_test, y_test = data\n",
    "    y_pred = model.predict(x_test, batch_size=30)\n",
    "    print('-' * 50)\n",
    "#     print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1)) / y_test.shape[0])\n",
    "#     print(y_pred)\n",
    "    y_pred = model.predict_classes(x_test)\n",
    "#     print(y_pred)\n",
    "    \n",
    "#     print(np.argmax(y_pred, 1))\n",
    "\n",
    "#     print(classification_report(np.argmax(y_test, axis=1), \n",
    "#                                 y_pred, target_names=target_names))\n",
    "    \n",
    "#     result = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "#     print( confusion_matrix(y_test, y_pred, 1) )\n",
    "    f1 = f1_score(np.argmax(y_test, 1), y_pred, average='macro')  \n",
    "    percision = precision_score(np.argmax(y_test, 1), y_pred, average='macro')  \n",
    "    recall = recall_score(np.argmax(y_test, 1), y_pred, average='macro')\n",
    "\n",
    "    return f1, percision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "batch_size=64\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "# evaluate using 10-fold cross validation\n",
    "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "\n",
    "models = {\n",
    "#     'not_learning' : not_learning() ,\n",
    "#     'create_model' : create_model(),\n",
    "#     'getRNN_classifier' : getRNN_classifier(),\n",
    "#     'simple_model' : simple_model(),\n",
    "#     'ker/as_conv' : keras_conv(),\n",
    "#     'RNN_model' : RNN_model(),\n",
    "#     'CNNTextClassification' : CNNTextClassification(),\n",
    "#     'LSTMModel' : LSTMModel(),\n",
    "#     'word_embedding_model' : word_embedding_model(),\n",
    "#     'sentiment_keras' : sentiment_keras(),\n",
    "#     'imbd_lstm' : imbd_lstm(),\n",
    "#     'GRUModel' : GRUModel(),\n",
    "#     'imbd_cnn' : imbd_cnn()\n",
    "}\n",
    "# def getModel(name):\n",
    "#     if(not_learning):\n",
    "#         return not_learning()\n",
    "#     else if('create_model' ):\n",
    "#         return create_model()\n",
    "#     else if('getRNN_classifier' ):\n",
    "#         return getRNN_classifier()\n",
    "#     else if('simple_model'):\n",
    "#          return simple_model()\n",
    "#     else if('keras_conv' ):\n",
    "#         return keras_conv()\n",
    "#     else if('RNN_model' ):\n",
    "#          return RNN_model(),\n",
    "#     else if('CNNTextClassification' ):\n",
    "#     #     'CNNTextClassification' : CNNTextClassification(),\n",
    "#     else if('LSTMModel'):\n",
    "#         return LSTMModel()\n",
    "#     else if('word_embedding_model' ):\n",
    "#         return word_embedding_model(),\n",
    "#     else if('sentiment_keras' ):\n",
    "#         return sentiment_keras(),\n",
    "#     else if('imbd_lstm' ):\n",
    "#          return imbd_lstm(),\n",
    "#     else if('GRUModel' ):\n",
    "#         'GRUModel' : GRUModel(),\n",
    "#         else if('create_model' ):\n",
    "# #     'imbd_cnn' : imbd_cnn()\n",
    "\n",
    "score = []\n",
    "\n",
    "\n",
    "for train_index, test_index in tqdm(skf.split(X, data_train['target'])):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    x_train, x_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    model =  GRUModel() \n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs, verbose=0,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "#         result = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    f1, percision, recall = test(model=model, data=(x_test, y_test))\n",
    "    score.append({'f1': f1, 'percision' : percision, 'recall' : recall})\n",
    "    print('F1: ', f1, ' percision: ', percision, ' recall: ', recall)\n",
    "    \n",
    "#     print(score_temp)\n",
    "#     score.append([name, score_temp])\n",
    "#     print('average percision over kfolds:', np.average([x['percision'] for x in score_temp]))\n",
    "\n",
    "print('average f1 over kfolds:', np.average([x['f1'] for x in score]))\n",
    "print('average percision over kfolds:', np.average([x['percision'] for x in score]))\n",
    "print('average recall over kfolds:', np.average([x['recall'] for x in score]))\n",
    "\n",
    "# print(score)\n",
    "df_result = pd.DataFrame(score)\n",
    "df_result.head()\n",
    "\n",
    "df_result.to_csv('data/NNResults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_result.plot()\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('average score over kfolds:', np.mean([x[1] for x in score]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
