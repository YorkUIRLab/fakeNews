{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import Bidirectional, TimeDistributed, GRU\n",
    "from keras.layers import LSTM, Input, Reshape, Concatenate, Flatten,Convolution1D\n",
    "from keras.layers import Conv1D, Conv2D, GlobalMaxPooling1D, MaxPooling1D, MaxPool2D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from os.path import expanduser, exists\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='whitegrid', palette='muted')\n",
    "# pd.set_option('display.max_colwidth', 300) #widen pandas rows display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_splits(X, y, max_words = 20000, MAX_SEQUENCE_LENGTH = 100, EMBEDDING_DIM = 300):\n",
    "    tokenizer = Tokenizer(num_words=max_words,lower=True, split=' ', \n",
    "                          filters='\"#%&()*+-/<=>@[\\\\]^_`{|}~\\t\\n',\n",
    "                          char_level=False, oov_token=u'<UNK>')\n",
    "\n",
    "    tokenizer.fit_on_texts(X)\n",
    "\n",
    "    X = tokenizer.texts_to_sequences(X)\n",
    "    Y = pd.get_dummies(y).values\n",
    "    # print(X[0])\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH,  padding=\"post\", truncating=\"post\")\n",
    "    # print(X[10])\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, \n",
    "                                                        random_state = 42)\n",
    "\n",
    "    # print(y_train[100])\n",
    "\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print('x_test shape:', x_test.shape)\n",
    "    print('y_train shape:', y_train.shape)\n",
    "    print('y_test shape:', y_test.shape)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['false' 'true']\n",
      "Found 17413 unique tokens.\n",
      "x_train shape: (4804, 100)\n",
      "x_test shape: (1202, 100)\n",
      "y_train shape: (4804, 2)\n",
      "y_test shape: (1202, 2)\n",
      "Found 17413 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "max_words = 20000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 100 \n",
    "\n",
    "from data_util import get_fake_or_real_news, get_politifact, get_fact_fake\n",
    "\n",
    "# x_train, x_test, y_train, y_test, word_index, labels, num_classes, X, y = get_fake_or_real_news()\n",
    "# x_train, x_test, y_train, y_test, word_index, labels, num_classes, X, y = get_politifact()\n",
    "# x_train, x_test, y_train, y_test, word_index, labels, num_classes, X, y = get_fact_fake()\n",
    "\n",
    "def to_label(cat):\n",
    "    return categories[cat]\n",
    "\n",
    "##### FakeNewsNet\n",
    "# categories = {'FakeNewsContent':'False', 'RealNewsContent':'True'}\n",
    "# data_train = pd.read_csv('data/fakeNewsNet.csv', sep='\\t')\n",
    "# data_train.dropna(how='any', inplace=True) \n",
    "# print(data_train['collected_by'].unique())\n",
    "# print(data_train['label'].unique())\n",
    "# print(data_train.isnull().values.any())\n",
    "# data_train['label'] = data_train['label'].apply(to_label)\n",
    "# X = data_train['text'].values\n",
    "# y = data_train['label'].values\n",
    "# labels = data_train['label'].unique()\n",
    "# num_classes = len(labels)\n",
    "# print(data_train['label'].value_counts())\n",
    "# x_train, x_test, y_train, y_test, word_index = get_splits(X, y)\n",
    "#########################################\n",
    "\n",
    "##### SNOPES\n",
    "data_train = pd.read_csv('data/snopes_processed.tsv', sep='\\t')\n",
    "categories = {True:u'true', False: u'false'}\n",
    "data_train['label'] = data_train['label'].apply(to_label)\n",
    "X = data_train['claim'].values.astype(str)\n",
    "y = data_train['label'].values\n",
    "labels = data_train['label'].unique()\n",
    "print(labels)\n",
    "num_classes = len(labels)\n",
    "x_train, x_test, y_train, y_test, word_index = get_splits(X, y)\n",
    "###################\n",
    "\n",
    "vocabulary_inv = dict((v, k) for k, v in word_index.items())\n",
    "print('Found %s unique tokens.' % len(vocabulary_inv))\n",
    "vocabulary_inv[0] = \"<PAD/>\"\n",
    "\n",
    "# x_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_confusion_matrix(model, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    class_labels = np.argmax(y_test, axis=1) \n",
    "    \n",
    "    print(metrics.classification_report(class_labels, y_pred.argmax(axis=1), \n",
    "                                        target_names=labels, digits=3))\n",
    "    \n",
    "    #Plot ROC curve\n",
    "    plot_ROC(class_labels, y_pred)\n",
    "    \n",
    "    cm = confusion_matrix(class_labels, \n",
    "                          y_pred.argmax(axis=1))\n",
    "    \n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "#     plt.figure(figsize=(12,8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_ROC(class_labels, y_pred):\n",
    "        \n",
    "    print(\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(class_labels, y_pred.argmax(axis=1)),\n",
    "                                           roc_auc_score(class_labels, y_pred[:, 1])))\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(class_labels, y_pred[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "#     print(fpr, tpr)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing glove.840B.300d.txt"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "KERAS_DATASETS_DIR = expanduser('~/.keras/datasets/')\n",
    "GLOVE_FILE = 'glove.840B.300d.txt'\n",
    "\n",
    "print(\"Processing\", GLOVE_FILE)\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(KERAS_DATASETS_DIR + GLOVE_FILE, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings: %d' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_layer():\n",
    "    count = 0\n",
    "    embedding_matrix = np.random.uniform(-1,0,(len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector[:EMBEDDING_DIM]\n",
    "            count += 1\n",
    "            \n",
    "    print('Word embeddings: %d' % len(embeddings_index))\n",
    "    print('found number of tokens in embedding space: ', count)\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "    return embedding_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GRU_2():\n",
    "    model = Sequential()\n",
    "    model.add(get_embedding_layer())\n",
    "    model.add(GRU(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "    model.add(GRU(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy', \n",
    "                  optimizer='Adagrad', metrics = ['accuracy'])\n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/eashish/bidirectional-gru-with-convolution/code\n",
    "\n",
    "def bidirectional_conv_GRU():\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH, ))\n",
    "    x =  get_embedding_layer()(sequence_input)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "    x = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    x = concatenate([avg_pool, max_pool]) \n",
    "    # x = Dense(128, activation='relu')(x)\n",
    "    # x = Dropout(0.1)(x)\n",
    "    preds = Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bidirectional_GRU():\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = get_embedding_layer()(sequence_input)\n",
    "    l_lstm = Bidirectional(GRU(100))(embedded_sequences)\n",
    "    preds = Dense(num_classes, activation='softmax')(l_lstm)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_LSTM():\n",
    "    model = Sequential()\n",
    "    model.add(get_embedding_layer())\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', \n",
    "                  optimizer=adam, metrics = ['acc'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE MODEL\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "\n",
    "##########\n",
    "filepath=\"data/weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=10)\n",
    "ra_val = RocAucEvaluation(validation_data=(x_test, y_test), interval = 1)\n",
    "callbacks_list = [ra_val,checkpoint, early]\n",
    "##########\n",
    "\n",
    "# model = bidirectional_conv_GRU()\n",
    "model = bidirectional_GRU()\n",
    "# model = simple_LSTM()\n",
    "# model = GRU_2()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "\n",
    "print('Train...')\n",
    "network_hist = model.fit(x_train, y_train, \n",
    "                         batch_size=batch_size, epochs=epochs,\n",
    "                         verbose=1, validation_data=(x_test, y_test),\n",
    "                         callbacks = callbacks_list,\n",
    "                         validation_split=0.2)\n",
    "\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "plot_history(network_hist)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(model, classes=labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def test(model, data):\n",
    "    x_test, y_test = data\n",
    "    y_pred = model.predict(x_test)\n",
    "    print('-' * 50)\n",
    "\n",
    "    class_labels = np.argmax(y_test, axis=1) \n",
    "#     print(class_labels[:10])\n",
    "#     print(y_pred.argmax(axis=1))\n",
    "    y_pred = y_pred.argmax(axis=1)\n",
    "    \n",
    "#     print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1)) / y_test.shape[0])\n",
    "#     print(y_pred)\n",
    "#     y_pred = model.predict_classes(x_test)\n",
    "#     print(y_pred)\n",
    "    \n",
    "#     print(np.argmax(y_pred, 1))\n",
    "\n",
    "#     print(classification_report(np.argmax(y_test, axis=1), \n",
    "#                                 y_pred, target_names=target_names))\n",
    "    \n",
    "#     result = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "#     print( confusion_matrix(y_test, y_pred, 1) )\n",
    "    f1 = f1_score(class_labels, y_pred.round(), average='macro')  \n",
    "    percision = precision_score(class_labels, y_pred.round(), average='macro')  \n",
    "    recall = recall_score(class_labels, y_pred.round(), average='macro')\n",
    "\n",
    "    return f1, percision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "epochs = 2\n",
    "batch_size=128\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "# evaluate using 10-fold cross validation\n",
    "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "\n",
    "score = []\n",
    "\n",
    "for train_index, test_index in tqdm(skf.split(X, y)):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    x_train, x_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    model =  bidirectional_conv_GRU() \n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs, verbose=0,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "#         result = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    f1, percision, recall = test(model=model, data=(x_test, y_test))\n",
    "    score.append({'f1': f1, 'percision' : percision, 'recall' : recall})\n",
    "    print('F1: ', f1, ' percision: ', percision, ' recall: ', recall)\n",
    "    \n",
    "#     print(score_temp)\n",
    "#     score.append([name, score_temp])\n",
    "#     print('average percision over kfolds:', np.average([x['percision'] for x in score_temp]))\n",
    "\n",
    "print('average f1 over kfolds:', np.average([x['f1'] for x in score]))\n",
    "print('average percision over kfolds:', np.average([x['percision'] for x in score]))\n",
    "print('average recall over kfolds:', np.average([x['recall'] for x in score]))\n",
    "\n",
    "# print(score)\n",
    "df_result = pd.DataFrame(score)\n",
    "df_result.head()\n",
    "\n",
    "df_result.to_csv('data/NNResults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_result.plot()\n",
    "df_result.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
